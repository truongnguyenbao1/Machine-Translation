# Machine-Translation
Sequence-to-sequence models are machine learning models that take a sequence input and generate a new output chain. These models are often based on the Transformer architecture, which is widely used in natural language processing (NLP) problems
